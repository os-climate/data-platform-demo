{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055e228d-c294-4bff-ace1-8470358f0a57",
   "metadata": {},
   "source": [
    "Run these in a notebook cell if you need to install onto your nb env\n",
    "```\n",
    "# 'capture' magic prevents long outputs from spamming your notebook\n",
    "%%capture pipoutput\n",
    "\n",
    "# For loading predefined environment variables from files\n",
    "# Typically used to load sensitive access credentials\n",
    "%pip install python-dotenv\n",
    "\n",
    "# Standard python package for interacting with S3 buckets\n",
    "%pip install boto3\n",
    "\n",
    "# Interacting with Trino and using Trino with sqlalchemy\n",
    "%pip install trino sqlalchemy sqlalchemy-trino\n",
    "\n",
    "# Pandas and parquet file i/o\n",
    "%pip install pandas pyarrow fastparquet\n",
    "\n",
    "# OS-Climate utilities to make data ingest easier\n",
    "%pip install osc-ingest-tools\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9915e4-cb4e-43ad-9865-4cea6d160b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load some standard environment variables from a dot-env file, if it exists.\n",
    "# If no such file can be found, does not fail, and so allows these environment vars to\n",
    "# be populated in some other way\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9eb6da-2292-4dcc-b025-e95b194ef691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ[\"S3_DEV_ENDPOINT\"],\n",
    "    aws_access_key_id=os.environ[\"S3_DEV_ACCESS_KEY\"],\n",
    "    aws_secret_access_key=os.environ[\"S3_DEV_SECRET_KEY\"],\n",
    ")\n",
    "bucket = s3.Bucket(os.environ[\"S3_DEV_BUCKET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67058349-ee99-41c0-80de-3ebb1d556372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "sqlstring = 'trino://{user}@{host}:{port}/'.format(\n",
    "    user = os.environ['TRINO_USER'],\n",
    "    host = os.environ['TRINO_HOST'],\n",
    "    port = os.environ['TRINO_PORT']\n",
    ")\n",
    "sqlargs = {\n",
    "    'auth': trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    'http_scheme': 'https'\n",
    "}\n",
    "engine = create_engine(sqlstring, connect_args = sqlargs)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f8501f-6145-4cc1-8573-892a63cccaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'demo'\n",
    "ingest_table = 'parquet_partitions_tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643cf26e-8c52-422d-b6c5-a5f802ed86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_columns = ['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00f05a-5e85-4212-9c93-b368d9051661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = [['2020', 10], ['2021', 15], ['2022', 14]]\n",
    "df1 = pd.DataFrame(data, columns = ['year', 'metric'])\n",
    "df1 = df1.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e58c0-4d1e-4a1a-8d9b-7c2ab3e92c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do: put this in osc-ingest-tools\n",
    "def enforce_partition_column_order(df, pcols, inplace=False):\n",
    "    cols = list(df.columns.values)\n",
    "    for c in pcols:\n",
    "        cols.remove(c)\n",
    "        cols.append(c)\n",
    "    if not inplace:\n",
    "        return df[cols]\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "        df[c] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f91a7d-271e-4a04-9b95-759d2f851741",
   "metadata": {},
   "outputs": [],
   "source": [
    "enforce_partition_column_order(df1, partition_columns, inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668e919-1fc6-441d-ae2f-be77b821b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "drop table if exists {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "qres = engine.execute(sql)\n",
    "print(qres.fetchall())\n",
    "\n",
    "# hive connector does not manage underlying files for you\n",
    "# so to truly drop a table you must manually remove underlying data files\n",
    "bucket.objects \\\n",
    "    .filter(Prefix=f'trino/{ingest_schema}/{ingest_table}/') \\\n",
    "    .delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def7855-7e32-4c7f-b0d6-2fcfac6e7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: add this utility to osc-ingest-tools\n",
    "import os\n",
    "def upload_directory_to_s3(path, bucket, prefix):\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            pfx = subdir.replace(path, prefix)\n",
    "            src = os.path.join(subdir, f)\n",
    "            dst = os.path.join(pfx, f)\n",
    "            #print(f'{src}  -->  {dst}')\n",
    "            bucket.upload_file(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf12db5-8ba9-4c1d-844b-a9868f82e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "tmp = f'/tmp/{ingest_table}'\n",
    "\n",
    "# pandas does not clean out destination directory for you:\n",
    "shutil.rmtree(tmp, ignore_errors=True)\n",
    "\n",
    "# tell pandas to write a directory tree, using partitions\n",
    "df1.to_parquet(tmp,\n",
    "               partition_cols=partition_columns,\n",
    "               index=False)\n",
    "\n",
    "# upload the tree onto S3\n",
    "# The previous call to `df.to_parquet` automatically creates unique filenames,\n",
    "# so any pre-existing data out on s3 is NOT overwritten.\n",
    "# Effectively this is an \"append\" operation on the corresponding trino DB\n",
    "# If you want to overwrite trino's data you will also have to remove\n",
    "# any pre-existing data out on S3 before you upload\n",
    "upload_directory_to_s3(tmp, bucket, f'trino/{ingest_schema}/{ingest_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa26a6-8002-4ab2-b859-45ce1eac224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in bucket.objects \\\n",
    "    .filter(Prefix=f'trino/{ingest_schema}/{ingest_table}/'):\n",
    "    print(e.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdaba9f-bcd3-4055-89c4-9e51ee985a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import osc_ingest_trino as osc\n",
    "columnschema = osc.create_table_schema_pairs(df1)\n",
    "\n",
    "tabledef = f\"\"\"\n",
    "create table if not exists {ingest_catalog}.{ingest_schema}.{ingest_table}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    partitioned_by = array{partition_columns},\n",
    "    external_location = 's3a://{bucket.name}/trino/{ingest_schema}/{ingest_table}/'\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58eb64-90a8-4abb-aef5-6da46a8d2e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "select * from {ingest_catalog}.{ingest_schema}.{ingest_table}\n",
    "\"\"\"\n",
    "df = pd.read_sql(sql, engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f04ebe-f645-4b8b-9b65-98605f56484d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
